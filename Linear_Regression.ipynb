{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Linear_Regression.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMTucNn8s/KNM9CJ+ZbGQP3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"N2ewrWsKqJmR","colab_type":"text"},"source":["- 모델을 학습시키기 위한 데이터는 파이토치의 텐서의 형태(torch.tensor)를 가지고 있어야 합니다\n"]},{"cell_type":"code","metadata":{"id":"LSoUsEF8U0Rk","colab_type":"code","colab":{}},"source":["import torch.nn\n","\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WmOwXZQMU0dK","colab_type":"text"},"source":["![스크린샷 2020-03-18 오후 8 14 00](https://user-images.githubusercontent.com/44131043/76955198-14c7da80-6955-11ea-80e4-5c58c9987b8d.png)\n","\n","- W = W - (알파* 기울기)\n","\n","![스크린샷 2020-03-18 오후 8 15 37](https://user-images.githubusercontent.com/44131043/76955287-3d4fd480-6955-11ea-9184-1b8be87bbb19.png)\n"]},{"cell_type":"markdown","metadata":{"id":"3UGu291JWjvA","colab_type":"text"},"source":["# pytorch _ linear reg"]},{"cell_type":"code","metadata":{"id":"fGkTTfx5XhXo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"outputId":"80485eb5-a7e9-490d-d918-388667f64296","executionInfo":{"status":"ok","timestamp":1584531709530,"user_tz":-540,"elapsed":1200,"user":{"displayName":"g1moon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZL5BQYZB4jmoREa2K_Tlx6hwQ1IwtV7mYVb34=s64","userId":"16307972960386931414"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","torch.manual_seed(1)\n","\n","#--------변수선언----------#\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n","\n","print(f'\\n---x_train:--- \\n {x_train}')\n","print(f'\\n---x_train.shape:--- \\n {x_train.shape}')\n","\n","#------가중치와 편향의 초기화------#\n","# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.\n","W = torch.zeros(1, requires_grad=True) # 1번쨰 size \n","b = torch.zeros(1, requires_grad= True) # \n","print(f'\\n--------W:------- \\n {W}')\n","print(W.shape) #1 \n","\n","#----가설세우기------\n","hypothesis = x_train * W + b\n","print(f'\\n-----hypothesis----:\\n {hypothesis}')\n","\n","#-----비용함수 선언-----\n","cost = torch.mean((hypothesis - y_train) **2)\n","print(f'\\n cost: {cost}')\n","\n","#---경사하강 구현-----\n","optimizer = optim.SGD([W,b], lr = 0.01) #학습 대상인 W와 b가 SGD의 입력이 됩니다.\n","    \n","    #gradient를 0으로 초기화 \n","optimizer.zero_grad()\n","    #비용 함수를 미분하여 gradient 계산\n","cost.backward()\n","    #W와 b를 업데이트\n","optimizer.step()\n","\n","\n","\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["\n","---x_train:--- \n"," tensor([[1.],\n","        [2.],\n","        [3.]])\n","\n","---x_train.shape:--- \n"," torch.Size([3, 1])\n","\n","--------W:------- \n"," tensor([0.], requires_grad=True)\n","torch.Size([1])\n","\n","-----hypothesis----:\n"," tensor([[0.],\n","        [0.],\n","        [0.]], grad_fn=<AddBackward0>)\n","\n"," cost: 18.66666603088379\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HhoUOEgbcxlZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"ab555325-2556-4814-e72d-4962603186e1","executionInfo":{"status":"ok","timestamp":1584531811451,"user_tz":-540,"elapsed":1267,"user":{"displayName":"g1moon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZL5BQYZB4jmoREa2K_Tlx6hwQ1IwtV7mYVb34=s64","userId":"16307972960386931414"}}},"source":["#전체 코드 \n","\n","# 데이터\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n","# 모델 초기화\n","W = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=0.01)\n","\n","nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\n","for epoch in range(nb_epochs + 1): #0~2000까지 \n","\n","    # H(x) 계산\n","    hypothesis = x_train * W + b\n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad() #gradient 0으로 초기화\n","    cost.backward() #비용 함수 미분해 gradient 계산\n","    optimizer.step() # w,b를 업데이트 \n","\n","    # 100번마다 로그 출력\n","    #item으로 출력을 합니다 \n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, W.item(), b.item(), cost.item()\n","        ))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Epoch    0/2000 W: 0.187, b: 0.080 Cost: 18.666666\n","Epoch  100/2000 W: 1.746, b: 0.578 Cost: 0.048171\n","Epoch  200/2000 W: 1.800, b: 0.454 Cost: 0.029767\n","Epoch  300/2000 W: 1.843, b: 0.357 Cost: 0.018394\n","Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011366\n","Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007024\n","Epoch  600/2000 W: 1.924, b: 0.174 Cost: 0.004340\n","Epoch  700/2000 W: 1.940, b: 0.136 Cost: 0.002682\n","Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001657\n","Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001024\n","Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000633\n","Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000391\n","Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000242\n","Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000149\n","Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000092\n","Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n","Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n","Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n","Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000013\n","Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n","Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J7mzbavvXz2-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P29FfS5odNAu","colab_type":"text"},"source":["# optimizer.zero_grad()가 필요한 이유\n","\n","- 파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있습니다. 예를 들어봅시다.\n","- 계속해서 미분값인 2가 누적되는 것을 볼 수 있습니다.   \n","그렇기 때문에 optimizer.zero_grad()를 통해 미분값을 __계속 0으로 초기화__ 시켜줘야 합니다\n"]},{"cell_type":"code","metadata":{"id":"fkSL4oAFdO4B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"cf9fec70-a855-4c1e-a49e-60bfc79c5bf6","executionInfo":{"status":"ok","timestamp":1584531834811,"user_tz":-540,"elapsed":868,"user":{"displayName":"g1moon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZL5BQYZB4jmoREa2K_Tlx6hwQ1IwtV7mYVb34=s64","userId":"16307972960386931414"}}},"source":["import torch\n","w = torch.tensor(2.0, requires_grad=True)\n","\n","nb_epochs = 20\n","for epoch in range(nb_epochs + 1):\n","\n","  #cost\n","  z = 2*w  \n","\n","#w로 미분\n","  z.backward() \n","  print('수식을 w로 미분한 값 : {}'.format(w.grad))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["수식을 w로 미분한 값 : 2.0\n","수식을 w로 미분한 값 : 4.0\n","수식을 w로 미분한 값 : 6.0\n","수식을 w로 미분한 값 : 8.0\n","수식을 w로 미분한 값 : 10.0\n","수식을 w로 미분한 값 : 12.0\n","수식을 w로 미분한 값 : 14.0\n","수식을 w로 미분한 값 : 16.0\n","수식을 w로 미분한 값 : 18.0\n","수식을 w로 미분한 값 : 20.0\n","수식을 w로 미분한 값 : 22.0\n","수식을 w로 미분한 값 : 24.0\n","수식을 w로 미분한 값 : 26.0\n","수식을 w로 미분한 값 : 28.0\n","수식을 w로 미분한 값 : 30.0\n","수식을 w로 미분한 값 : 32.0\n","수식을 w로 미분한 값 : 34.0\n","수식을 w로 미분한 값 : 36.0\n","수식을 w로 미분한 값 : 38.0\n","수식을 w로 미분한 값 : 40.0\n","수식을 w로 미분한 값 : 42.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ov8o8y_qdQQg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aRSyAzhLeIXz","colab_type":"text"},"source":["# autogard\n","- 사 하강법 코드를 보고있으면 requires_grad=True, backward() 등이 나옵니다.\n","- 는 파이토치에서 제공하고 있는 자동 미분(Autograd) 기능을 수행하고 있는 것입니다 "]},{"cell_type":"markdown","metadata":{"id":"rHEIErkeeMIT","colab_type":"text"},"source":["# autograd 실습\n"]},{"cell_type":"code","metadata":{"id":"Kkd0SR8veaa-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"af6bce5e-3f28-41dd-a92e-1ebceb498fc5","executionInfo":{"status":"ok","timestamp":1584532278903,"user_tz":-540,"elapsed":668,"user":{"displayName":"g1moon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZL5BQYZB4jmoREa2K_Tlx6hwQ1IwtV7mYVb34=s64","userId":"16307972960386931414"}}},"source":["import torch\n","\n","#이때 required_grad를 True로 \n","#이 텐서에 대한 기울기를 저장하겠다는 의미입니다. \n","#이렇게 하면 w.grad에 w에 대한 미분값이 저장됩니다.\n","\n","w = torch.tensor(2.0, requires_grad=True)\n","\n","#수식정의\n","y= w**2\n","z= 2*y + 5\n","\n","#해당 수식을 w에 대해 미분\n","#.backward()를 호출해 해당 수식의 w에대한 기울기 계싼\n","z.backward()\n","\n","print('수식을 w로 미분한 값 : {}'.format(w.grad))\n"],"execution_count":26,"outputs":[{"output_type":"stream","text":["수식을 w로 미분한 값 : 8.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AsVMreC0e8uV","colab_type":"text"},"source":["# multivariable linear reg"]},{"cell_type":"code","metadata":{"id":"3hnEHnhkfmhn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":394},"outputId":"afa8a2a5-bfa2-45dd-a4b3-c526aa5e606f","executionInfo":{"status":"ok","timestamp":1584532755722,"user_tz":-540,"elapsed":753,"user":{"displayName":"g1moon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZL5BQYZB4jmoREa2K_Tlx6hwQ1IwtV7mYVb34=s64","userId":"16307972960386931414"}}},"source":["x_train  =  torch.FloatTensor([[73,  80,  75], \n","                               [93,  88,  93], \n","                               [89,  91,  90], \n","                               [96,  98,  100],   \n","                               [73,  66,  70]])  \n","\n","y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n","\n","\n","# 모델 초기화\n","W = torch.zeros((3, 1), requires_grad=True)\n","    #print(x_train.shape) : [5,3]\n","    #print(W.shape): [3,1]\n","\n","b = torch.zeros(1, requires_grad=True)\n","\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=1e-5)\n","\n","#\n","nb_epochs = 20\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n","    # x_train * W\n","    hypothesis = x_train.matmul(W) + b \n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad() # optimizer 0세팅\n","    cost.backward() #gradient 계산\n","    optimizer.step() # 업데이트 \n","\n","    # 100번마다 로그 출력\n","    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n","        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n","    ))"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n","Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n","Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n","Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n","Epoch    4/20 hypothesis: tensor([137.7967, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936096\n","Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371071\n","Epoch    6/20 hypothesis: tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812]) Cost: 29.758249\n","Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8509, 193.6753, 137.6805]) Cost: 10.445267\n","Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391237\n","Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493121\n","Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n","Epoch   11/20 hypothesis: tensor([152.5485, 183.3609, 180.6640, 196.7389, 139.8602]) Cost: 1.710552\n","Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651416\n","Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632369\n","Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625924\n","Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623420\n","Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622152\n","Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621261\n","Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0661, 140.0963]) Cost: 1.620501\n","Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9665, 197.0686, 140.0985]) Cost: 1.619757\n","Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) Cost: 1.619046\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8HT89YEwfml4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWHR-MbdfYUa","colab_type":"text"},"source":[""]}]}